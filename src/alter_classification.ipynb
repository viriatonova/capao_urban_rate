{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capao Urban Rate\n",
    "\n",
    "This notebook aims to estimate the current urban growth rate for the place: Vale do Cap√£o, Palmeiras, BA, Brasil. By using Google Earth satellite images and analyzing RGB pixel data, we can determine the growth rate in areas featuring houses, roads, construction sites, or where forests have been cleared for humans uses. For this case study, we have chosen COPERNICUS satellite images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geemap as geemap\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "from utils.contants import PROJECT\n",
    "from utils.features import get_coordinates\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "ee.Authenticate()\n",
    "ee.Initialize(project=PROJECT)\n",
    "geemap.ee_initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Colletion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place location and geometry of interest\n",
    "geo_point = ee.Geometry.Point([-41.501150593949305,-12.609558240448216])\n",
    "geo_place = ee.Geometry.Polygon([\n",
    "    [-41.51551178850023,-12.571410698153873],\n",
    "    [-41.51392392076341,-12.575557425201367],\n",
    "    [-41.51349476732103,-12.596499467766318],\n",
    "    [-41.516777307708075,-12.605673846986134],\n",
    "    [-41.513816148955634,-12.613023835365905],\n",
    "    [-41.510018140990546,-12.61471995657374],\n",
    "    [-41.5042460271905,-12.622069685275841],\n",
    "    [-41.49767997952204,-12.633585929553394],\n",
    "    [-41.495040685851386,-12.64577164489336],\n",
    "    [-41.49165037365656,-12.65577940923046],\n",
    "    [-41.48326410447515,-12.654326569148258],\n",
    "    [-41.482418542332205,-12.629461766530518],\n",
    "    [-41.484178071445974,-12.62054181330977],\n",
    "    [-41.486409669346365,-12.610658309543654],\n",
    "    [-41.4919457487531,-12.603664243777065],\n",
    "    [-41.4923984315149,-12.595572883050577],\n",
    "    [-41.48810689709107,-12.581960700605178],\n",
    "    [-41.48832147381226,-12.568473457194342],\n",
    "    [-41.512010743831794,-12.568347796317699],\n",
    "    [-41.51551178850023,-12.571410698153873]\n",
    "])\n",
    "\n",
    "# Pre process urban features\n",
    "df = pd.read_csv('./data/urban_features.csv')\n",
    "raw_urban = df['.geo'].tolist()\n",
    "urban = [get_coordinates(i) for i in raw_urban]\n",
    "urban_features_list = [\n",
    "  ee.Feature(ee.Geometry.Point(urban[i][0], urban[i][1])) for i in range(len(urban))\n",
    "]\n",
    "\n",
    "# Pre process vegetation features\n",
    "df = pd.read_csv('./data/vegetation_feature.csv')\n",
    "raw_vegetation = df['.geo'].tolist()\n",
    "vegetation = [get_coordinates(i) for i in raw_vegetation]\n",
    "vegetation_features_list = [\n",
    "  ee.Feature(ee.Geometry.Point(vegetation[i][0], vegetation[i][1])) for i in range(len(vegetation))\n",
    "]\n",
    "\n",
    "# Feature collections\n",
    "urban_features = ee.FeatureCollection(urban_features_list)\n",
    "vegetation_features = ee.FeatureCollection(vegetation_features_list)\n",
    "\n",
    "image = (\n",
    "  ee.Image(\n",
    "    ee.ImageCollection(\"COPERNICUS/S2_SR_HARMONIZED\")\n",
    "      .filterBounds(geo_point)\n",
    "      .filterDate('2023-01-01', '2023-12-01')\n",
    "      .sort('CLOUDY_PIXEL_PERCENTAGE')\n",
    "      .first()\n",
    "  )\n",
    ").clip(geo_place)\n",
    "\n",
    "bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = geemap.Map(center=[-12.609558240448216,-41.501150593949305], zoom=15)\n",
    "\n",
    "# Adjust this value to increase or decrease the contrast\n",
    "vis_params = {\"bands\": ['B2', 'B3', 'B4'],  min: 0, max: 2000, \"gamma\": 3.5}\n",
    "map.add_layer(image, vis_params, 'Polygon')\n",
    "\n",
    "# Display the map\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('./data/dataset_bands_3.csv')\n",
    "df = pd.read_csv('./data/dataset_bands_6.csv')\n",
    "\n",
    "# dataset = df[['system:index', 'B2', 'B3', 'B4']].copy()\n",
    "dataset = df[['system:index', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']].copy()\n",
    "\n",
    "dataset = dataset.replace(regex=r'1_\\d+_\\d+', value=0) # Urban Class 0\n",
    "dataset = dataset.replace(regex=r'2_\\d+_\\d+', value=1) # Urban Class 1\n",
    "dataset.rename(columns={'system:index': 'class'}, inplace=True)\n",
    "dataset[110:130]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the data to be in the range (0, 1)\n",
    "scaler = MinMaxScaler()\n",
    "dataset_normalized = scaler.fit_transform(dataset)\n",
    "\n",
    "# PCA to reduce dimensions from 3 to 2\n",
    "pca = PCA(n_components=2)\n",
    "data_reduced = pca.fit_transform(dataset_normalized)\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "clusters = kmeans.fit_predict(data_reduced)\n",
    "\n",
    "# Visualize clusters\n",
    "plt.scatter(data_reduced[:, 0], data_reduced[:, 1], c=clusters, cmap='viridis', marker='o')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('Clustered Data in 2D')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set the outlier threshold\n",
    "Q1 = np.percentile(dataset_normalized, 25, axis=0)\n",
    "Q3 = np.percentile(dataset_normalized, 75, axis=0)\n",
    "IQR = Q3 - Q1\n",
    "step = 1.5 * IQR\n",
    "\n",
    "# Check for outliers\n",
    "outliers = (dataset_normalized < (Q1 - step)) | (dataset_normalized > (Q3 + step))\n",
    "outliers_row = np.any(outliers, axis=1)\n",
    "outlier_indices = np.where(outliers_row)\n",
    "\n",
    "# Calculate the percentage of outliers\n",
    "outlier_count = np.sum(outliers_row)\n",
    "total_points = dataset_normalized.shape[0]\n",
    "outlier_percentage = f'{round((outlier_count / total_points) * 100, 3)}%'\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = ['red' if i in outlier_indices[0] else 'blue' for i in range(dataset_normalized.shape[0])]\n",
    "plt.scatter(dataset_normalized[:, 0], dataset_normalized[:, 1], c=colors, alpha=0.5)\n",
    "plt.xlabel('Normalized Red')\n",
    "plt.ylabel('Normalized Green')\n",
    "plt.title('Outliers in Normalized RGB Data (Red vs. Green)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print indices of outliers\n",
    "print(\"Indices of outlier data points:\", outlier_indices[0], outlier_percentage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean dataset from outliers\n",
    "dataset_cleaned = dataset[~outliers_row]\n",
    "\n",
    "# Set the outlier threshold for 3 bands\n",
    "# features = dataset[['B2', 'B3', 'B4']]\n",
    "# label = dataset['class']\n",
    "\n",
    "# Set the outlier threshold for 6 bands\n",
    "features = dataset[['B2', 'B3', 'B4', 'B5', 'B6', 'B7']]\n",
    "label = dataset['class']\n",
    "\n",
    "# features = dataset_cleaned[['B2', 'B3', 'B4']]\n",
    "# label = dataset_cleaned['class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, label, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", sk.metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Prectats import hmeanision:\", sk.metrics.precision_score(y_test, y_pred, average='macro'))\n",
    "print(\"Recall:\", sk.metrics.recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1 Score:\", sk.metrics.f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", sk.metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Prectats import hmeanision:\", sk.metrics.precision_score(y_test, y_pred, average='macro'))\n",
    "print(\"Recall:\", sk.metrics.recall_score(y_test, y_pred, average='macro'))\n",
    "print(\"F1 Score:\", sk.metrics.f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cofunsion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "disp = sk.metrics.confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(disp, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "\n",
    "classes = [0, 1] \n",
    "tick_marks = np.arange(len(classes))\n",
    "\n",
    "plt.xticks(tick_marks, classes)\n",
    "plt.yticks(tick_marks, classes)\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "\n",
    "for i in range(disp.shape[0]):\n",
    "    for j in range(disp.shape[1]):\n",
    "        plt.text(j, i, disp[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if disp[i, j] > disp.max() / 2. else \"black\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_classified = image.select(['B2', 'B3', 'B4']).classify(clf)\n",
    "vis_params = {\n",
    "    'min': 0,\n",
    "    'max': 2000,\n",
    "    'palette': ['blue', 'green', 'red'],\n",
    "    'gamma': gamma\n",
    "}\n",
    "map.addLayer(place_classified, vis_params=vis_params, name='Classified');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capao-urban-rate-3l7dZDhn-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
